{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e51364b-33c1-4e49-b482-820fe25f3bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ALPHAVANTAGE_API_KEY? True\n"
     ]
    }
   ],
   "source": [
    "# Cell: Imports + utilities + setup\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Any\n",
    "import datetime as dt\n",
    "import urllib.robotparser\n",
    "\n",
    "# create data/raw if not exists\n",
    "DATA_RAW = pathlib.Path(\"data/raw\")\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load .env (must run before using ALPHA_KEY)\n",
    "load_dotenv()\n",
    "ALPHA_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "print(\"Loaded ALPHAVANTAGE_API_KEY?\", bool(ALPHA_KEY))\n",
    "\n",
    "# --- safe timestamp & filename helpers ---\n",
    "def safe_stamp() -> str:\n",
    "    # UTC timestamp (no spaces), safe for filenames\n",
    "    return dt.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def safe_filename(prefix: str, meta: Dict[str, Any]) -> str:\n",
    "    # meta is a dict; sanitize values (no spaces), limit lengths\n",
    "    mid = \"_\".join([f\"{k}-{str(v).replace(' ', '-')[:20]}\" for k, v in meta.items()])\n",
    "    return f\"{prefix}_{mid}_{safe_stamp()}.csv\"\n",
    "\n",
    "# --- improved validate_df (structured messages) ---\n",
    "def validate_df(df: pd.DataFrame, required_cols: List[str], dtypes_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    msgs = {}\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        msgs['missing_cols'] = f\"Missing columns: {missing}\"\n",
    "    for col, dtype in dtypes_map.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if dtype == 'datetime64[ns]':\n",
    "                    pd.to_datetime(df[col])\n",
    "                elif dtype == 'float':\n",
    "                    pd.to_numeric(df[col])\n",
    "            except Exception as e:\n",
    "                msgs[f'dtype_{col}'] = f\"Failed to coerce {col} to {dtype}: {e}\"\n",
    "    na_counts = df.isna().sum().sum()\n",
    "    msgs['na_total'] = f\"Total NA values: {na_counts}\"\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e6cd78-38e7-4f68-a814-973999d4120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bobli\\AppData\\Local\\Temp\\ipykernel_42736\\3310172685.py:4: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_api = yf.download(SYMBOL, period=\"6mo\", interval=\"1d\", progress=False).reset_index()[['Date','Close']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API validation: {'na_total': 'Total NA values: 0'}\n",
      "Saved: data\\raw\\api_source-yfinance_symbol-AAPL_20250818_005616.csv\n"
     ]
    }
   ],
   "source": [
    "use_alpha = bool(ALPHA_KEY)\n",
    "import yfinance as yf\n",
    "SYMBOL='AAPL'\n",
    "df_api = yf.download(SYMBOL, period=\"6mo\", interval=\"1d\", progress=False).reset_index()[['Date','Close']]\n",
    "df_api.columns = ['date','adj_close']\n",
    "df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "df_api['adj_close'] = pd.to_numeric(df_api['adj_close'], errors='coerce')\n",
    "\n",
    "# final clean\n",
    "df_api = df_api.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# validate\n",
    "msgs_api = validate_df(df_api, required_cols=['date','adj_close'],\n",
    "                       dtypes_map={'date':'datetime64[ns]','adj_close':'float'})\n",
    "print(\"API validation:\", msgs_api)\n",
    "fname = safe_filename(prefix=\"api\", meta={\"source\":  \"yfinance\", \"symbol\": SYMBOL})\n",
    "out_path = DATA_RAW / fname\n",
    "df_api.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dad3978-e90d-4a2d-a7f8-ae1d9abf6e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Company', 'Contact', 'Country'], ['Alfreds Futterkiste', 'Maria Anders', 'Germany'], ['Centro comercial Moctezuma', 'Francisco Chang', 'Mexico'], ['Ernst Handel', 'Roland Mendel', 'Austria'], ['Island Trading', 'Helen Bennett', 'UK'], ['Laughing Bacchus Winecellars', 'Yoshi Tannamuri', 'Canada'], ['Magazzini Alimentari Riuniti', 'Giovanni Rovelli', 'Italy']]\n"
     ]
    }
   ],
   "source": [
    "SCRAPE_URL = \"https://www.w3schools.com/html/html_tables.asp\"  # replace with permitted page\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\"}\n",
    "\n",
    "resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "table = soup.find('table')\n",
    "rows = []\n",
    "for tr in table.find_all('tr'):\n",
    "    cells = [td.get_text(strip=True) for td in tr.find_all(['td','th'])]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "    # assume first row is header\n",
    "print(rows)\n",
    "header, *data = rows\n",
    "df_scrape = pd.DataFrame(data, columns=header) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7950ecab-9cac-4803-9a17-38a2d31f47d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'na_total': 'Total NA values: 0'}\n",
      "Saved: data\\raw\\scrape_site-example_table-markets_20250818_005648.csv\n"
     ]
    }
   ],
   "source": [
    "msgs2 = validate_df(df_scrape, required_cols=list(df_scrape.columns), dtypes_map={})\n",
    "print(msgs2)\n",
    "\n",
    "fname2 = safe_filename(prefix=\"scrape\", meta={\"site\": \"example\", \"table\": \"markets\"})\n",
    "out_path2 = DATA_RAW / fname2\n",
    "df_scrape.to_csv(out_path2, index=False)\n",
    "print(\"Saved:\", out_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c6c61-084f-4071-bad2-b75e3f7acdb4",
   "metadata": {},
   "source": [
    "# Data acquisition â€” sources & validation (fill-in)\n",
    "\n",
    "## Data sources\n",
    "- API:\n",
    "  - Source: AlphaVantage (if API key provided) OR yfinance fallback.\n",
    "  - Endpoint / ticker: SYMBOL = AAPL  (change if needed).\n",
    "  - Params used: function=TIME_SERIES_DAILY_ADJUSTED, outputsize=compact (AlphaVantage) / period=6mo (yfinance)\n",
    "\n",
    "- Scrape:\n",
    "  - Scrape URL: <PUT YOUR SCRAPE_URL HERE>\n",
    "  - Table: first `<table>` on page (you may choose a more precise selector).\n",
    "  - Request headers: custom User-Agent \"AFE-Course-Notebook/1.0 (contact: instructor@example.edu)\"\n",
    "  - Robots.txt: checked programmatically before scraping.\n",
    "\n",
    "## Validation logic\n",
    "- Required columns: API must have `date` and `adj_close`. Scrape requires the columns found in the table.\n",
    "- Dtype checks: `date` -> datetime64[ns]; `adj_close` / price-like columns -> numeric (float).\n",
    "- NA checks: report total NA counts for early diagnosis.\n",
    "- Shape: saved for record in validation info.\n",
    "\n",
    "## Files saved\n",
    "- Saved raw CSVs in `data/raw/`:\n",
    "  - `<api_...>.csv`\n",
    "  - `<scrape_...>.csv`\n",
    "\n",
    "## .env & secrets\n",
    "- `.env` (contains ALPHAVANTAGE_API_KEY) **MUST NOT** be committed to Git.\n",
    "- Add `.env` to `.gitignore`.\n",
    "\n",
    "## Assumptions & Risks\n",
    "- Assumes AlphaVantage returns a standard \"Time Series\" JSON structure. Risk: rate limiting (AlphaVantage returns 'Note' on limit).\n",
    "- Assumes the scraped page allows crawling via robots.txt and that the first `<table>` is the desired data; risk: page layout differences or nested tables.\n",
    "- Demo fallback is included to ensure the notebook runs even if the external sources are blocked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f29e3-8d28-4001-8f30-999103775248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fe-course)",
   "language": "python",
   "name": "fe-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
