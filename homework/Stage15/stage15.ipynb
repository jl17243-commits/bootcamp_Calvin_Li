{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf23ecfb-0c6c-4a28-acfc-925cdf089b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-30 15:27:42,444 | INFO | Folders ready at C:\\Users\\Bobli\\bootcamp_Calvin_Li\\homework\\Stage15\\stage15_demo\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, json,logging,time,sys\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict,List,Callable\n",
    "\n",
    "BASE=Path.cwd()/'stage15_demo'\n",
    "DATA=BASE/'data'\n",
    "LOGS=BASE/'logs'\n",
    "REPORTS=BASE/'reports'\n",
    "for p in [DATA,LOGS,REPORTS]:\n",
    "    p.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout),\n",
    "             logging.FileHandler(LOGS/'pipeline.log')]\n",
    ")\n",
    "logger=logging.getLogger(__name__)\n",
    "logger.info('Folders ready at %s',BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0be7fd-f941-4bdd-aa96-5795830fccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskResult:\n",
    "    name:str\n",
    "    ok:bool\n",
    "    out_path:str |None=None\n",
    "    message:str =''\n",
    "def write_json(path,obj):\n",
    "    path.parent.mkdir(parents=True,exist_ok=True)\n",
    "    path.write_text(json.dumps(obj,indent=2))\n",
    "def checkpoint_exists(path):\n",
    "    return path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63ee1f0-7017-44a9-849f-d7359ab0f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_prices(output_path):\n",
    "    logger.info('[ingest_prices] start')\n",
    "    time.sleep(0.1)\n",
    "    data={'timestamp': datetime.utcnow().isoformat(), 'prices': [100.0, 101.2, 99.8, 102.1]}\n",
    "    write_json(output_path,data)\n",
    "    logger.info('[clean_prices] wrote %s',output_path)\n",
    "    return TaskResult('',True,str(output_path))\n",
    "\n",
    "def clean_prices(input_path,output_path):\n",
    "    logger.info('[clean_prices] start')\n",
    "    raw=json.loads(Path(input_path).read_text())\n",
    "    prices=[p for p in raw['prices'] if p is not None]\n",
    "    cleaned={'timestamp':raw['timestamp'],'prices':prices,'mean':sum(prices)/len(prices)}\n",
    "    write_json(output_path,cleaned)\n",
    "    logger.info('[clean_prices] wrote %s',output_path)\n",
    "    return TaskResult('clean_prices',True,str(output_path))\n",
    "\n",
    "def fit_dummy_model(clean_path,model_path):\n",
    "    logger.info('[fit_dummy_model] start')\n",
    "    clean=json.loads(Path(clean_path).read_text())\n",
    "    model={'type': 'mean_predictor', 'mean': clean['mean']}\n",
    "    write_json(model_path,model)\n",
    "    logger.info('[fit_dummy_model] wrote %s', model_path)\n",
    "    return TaskResult('fit_dummy_model', True, str(model_path))\n",
    "\n",
    "def generate_report(model_path,report_path):\n",
    "    logger.info('[generate_report] start')\n",
    "    model = json.loads(Path(model_path).read_text())\n",
    "    text = f'Report generated at {datetime.utcnow().isoformat()}\\nModel: {model}\\n'\n",
    "    report_path.write_text(text)\n",
    "    logger.info('[generate_report] wrote %s', report_path)\n",
    "    return TaskResult('generate_report', True, str(report_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2413bca3-f656-4e56-952e-0c35204860ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-30 16:23:08,056 | INFO | Pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "DAG={'ingest':[],\n",
    "    'clean':['ingest'],\n",
    "    'fit':['clean'],\n",
    "    'report':['fit']}\n",
    "def topo_sort(dag):\n",
    "    pending={k:set(v) for k,v in dag.items()}\n",
    "    done=[]\n",
    "    while pending:\n",
    "        ready=[k for k,deps in pending.items() if not deps]\n",
    "        if not ready:\n",
    "            raise ValueError('Cycle detected or unsatisfied deps')\n",
    "        for r in ready:\n",
    "            done.append(r)\n",
    "            pending.pop(r)\n",
    "            for deps in pending.values():\n",
    "                deps.discard(r)\n",
    "    return done\n",
    "\n",
    "def retry(n_tries=3,delay=0.2):\n",
    "    def wrap(fn,*args,**kwargs):\n",
    "        for i in range(1,n_tries+1):\n",
    "            try:\n",
    "                return fn(*args,**kwargs)\n",
    "            except Exception as e:\n",
    "                logger.exception('Attempt %s failed: %s',i,e)\n",
    "                time.sleep(delay*i)\n",
    "            raise RuntimeError(f'Task failed after {n_tries} attempts')\n",
    "    return wrap\n",
    "\n",
    "def run_pipeline(skip_existing=True):\n",
    "    ingest_out=DATA/'prices_raw.json'\n",
    "    clean_out=DATA/'prices_clean.json'\n",
    "    model_out=DATA/'model.json'\n",
    "    report_out=REPORTS/'report.txt'\n",
    "    order=topo_sort(DAG)\n",
    "    logger.info('Execution order:%s',order)\n",
    "\n",
    "    if not (skip_existing and checkpoint_exists(ingest_out)):\n",
    "        retry()(ingest_prices, ingest_out)\n",
    "    else:\n",
    "        logger.info('[ingest] checkpoint hit: %s', ingest_out)\n",
    "\n",
    "    if not (skip_existing and checkpoint_exists(clean_out)):\n",
    "        retry()(clean_prices, ingest_out, clean_out)\n",
    "    else:\n",
    "        logger.info('[clean] checkpoint hit: %s', clean_out)\n",
    "\n",
    "    if not (skip_existing and checkpoint_exists(model_out)):\n",
    "        retry()(fit_dummy_model, clean_out, model_out)\n",
    "    else:\n",
    "        logger.info('[fit] checkpoint hit: %s', model_out)\n",
    "\n",
    "    retry()(generate_report, model_out, report_out)\n",
    "    return str(report_out)\n",
    "\n",
    "logger.info('Pipeline ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ff48fb-8960-4db8-bc43-adcfcf30108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-30 16:28:46,732 | INFO | Use --run-all to execute the full demo pipeline.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description='Stage15 pipeline demo')\n",
    "    parser.add_argument('--run-all', action='store_true', help='Run the full pipeline')\n",
    "    parser.add_argument('--no-skip', action='store_true', help='Ignore checkpoints')\n",
    "    args = parser.parse_args(argv)\n",
    "    if args.run_all:\n",
    "        out = run_pipeline(skip_existing=not args.no_skip)\n",
    "        logger.info('Report at: %s', out)\n",
    "    else:\n",
    "        logger.info('Use --run-all to execute the full demo pipeline.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(['--no-skip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2ba49-dc0e-468c-a23d-c2e8286c5627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fe-course)",
   "language": "python",
   "name": "fe-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
